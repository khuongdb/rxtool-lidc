{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0fea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from diffdrr.drr import DRR\n",
    "from diffdrr.data import load_example_ct\n",
    "from diffdrr.visualization import plot_drr\n",
    "\n",
    "## library to read lung cancer database\n",
    "from sqlalchemy import func # required to query the db\n",
    "import thirdparty.pylidc.pylidc as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d3d7c-1ec4-4623-87e1-a3e27b074cc6",
   "metadata": {},
   "source": [
    "### Read Lung Cancer Database files\n",
    "Follow instructions at:\n",
    "\n",
    "https://pylidc.github.io/\n",
    "\n",
    "When running the code below, if getting errors such as np.bool not found or np.int not found, change the sources as follows\n",
    "\n",
    "- replace *np.int* by *int*\n",
    "\n",
    "- replace *np.bool* by *bool*\n",
    "\n",
    "First we need to set the path to the LIDC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c72435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder with Lung Cancer Dataset of CT volumes \n",
    "LIDC_DBB_folder=\"//home/ensai/Documents/msd06-1-smart-data-project/rxtools/data/lungCT/LIDC-IDRI/\"\n",
    "# create .pylidcrc file in ~/\n",
    "os.system(\"echo [dicom] > ~/.pylidcrc\")\n",
    "os.system(f'echo path={LIDC_DBB_folder} >> ~/.pylidcrc')\n",
    "os.system(\"echo warn=True >> ~/.pylidcrc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63aa024-f5ca-44cb-8611-e0b65ae97d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to generate the complete dataset, the code below has to be looped over all patients IDs\n",
    "pid = 'LIDC-IDRI-0001' # patient ID \n",
    "scan = pl.query(pl.Scan).filter(pl.Scan.patient_id == pid).first() # scan class instance for patient ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2986c433-aa41-4e31-85d7-66a3b8add702",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = scan.to_volume() # creating numpy array with scan volume\n",
    "print(f'Scan: {scan}')\n",
    "print(f'Scan spacing: {scan.pixel_spacing}')\n",
    "\n",
    "vol_shape=vol.shape\n",
    "\n",
    "print(f'Volume shape: {vol_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d33f71",
   "metadata": {},
   "source": [
    "\n",
    "`pixel_spacing` - attribute of scan object. \n",
    "\n",
    "float â€“ Dicom attribute (0028,0030). This is normally two values. All scans in the LIDC have equal resolutions in the transverse plane, so only one value is used here.\n",
    "\n",
    "## Annotation clustering\n",
    "\n",
    "The scan can have multiple annotations, but we several annotation can refer to the same nodule. We need to use annotation clustering to differentiate multiple nodules. This can be determined using the `pylidc.Scan.cluster_annotations()` method, which uses a distance function to create an adjancency graph to determine which annotations refer to the same nodule in a scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d94a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USe annotation clustering to differentiate nodules\n",
    "\n",
    "nods = scan.cluster_annotations()\n",
    "\n",
    "print(f\"{scan} has {len(nods)} nodules.\")\n",
    "\n",
    "for i, nod in enumerate(nods):\n",
    "    print(f\"Nodule {i}: has {len(nod)} annotation\")\n",
    "    for ann in nod: \n",
    "        print(f\"\\t{ann}\")\n",
    "\n",
    "scan.visualize(annotation_groups=nods)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3da39e4-edf7-48a8-a548-0f338d34176e",
   "metadata": {},
   "source": [
    "### Visualize mid slice of CT volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f6007f-6c9f-427f-80d0-404fb014d4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(vol[:, :, round(vol_shape[2]/2)].squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee2084c-0e75-4da9-b919-e4a91e36d62b",
   "metadata": {},
   "source": [
    "## Compute x-ray projections and create Annotations Gold Standard\n",
    "\n",
    "Compute x-ray projections from CT volumes. Also, create a volume with the annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d5a25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### parameters of the geometry of projection\n",
    "# We use diff DRR made to have differentiable DRR from CT volumes. Diff DRR is customized for cone-beam CTs\n",
    "# https://github.com/eigenvivek/DiffDRR\n",
    "\n",
    "sdr = 750  ## source-to-detector radius \n",
    "height = 1024 ### size of the projected image\n",
    "delx = (4*200)/height\n",
    "spacing = np.array([scan.pixel_spacing,scan.pixel_spacing, scan.slice_spacing])\n",
    "print(f\"CT voxel resolution is {spacing} mm\")\n",
    "\n",
    "# Initialize the DRR module for generating synthetic X-rays\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Found torch device is {device}\")\n",
    "\n",
    "### poses of camera to create projections ### this provides the geometric constraint\n",
    "### Diff DRR being created for a cone beam CT, lets try these\n",
    "# Set the camera pose with rotation (yaw, pitch, roll) and translation (x, y, z)\n",
    "#rotation = torch.tensor([[torch.pi, 0.0, torch.pi / 2]], device=device)\n",
    "rot_x=0.0\n",
    "rot_y=0.0\n",
    "rot_z=3*torch.pi/2\n",
    "rotation = torch.tensor([[rot_x, rot_y, rot_z]], device=device)\n",
    "\n",
    "bx, by, bz = torch.tensor(vol.shape) * torch.tensor(spacing) / 2 ### center of the CT volume in mm\n",
    "translation = torch.tensor([[bx, by, bz]], device=device) ### TO CHECK how this defines the camera pose according to the diffDRR package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c65858",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.set_per_process_memory_fraction(0.8, 0)  # Use 80% of GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ebdc0-fbe5-4473-8955-2c5dd5a27098",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create drr object from CT\n",
    "drr = DRR(\n",
    "    vol,      # The CT volume as a numpy array\n",
    "    spacing,     # Voxel dimensions of the CT\n",
    "    sdr=sdr,   # Source-to-detector radius (half of the source-to-detector distance)\n",
    "    height=height,  # Height of the DRR (if width is not seperately provided, the generated image is square)\n",
    "    delx=delx,    # Pixel spacing (in mm)\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "### create drr images using drr object for N_views. Each view is a rotation around the x axis of 360Â°/N_views from 0Â°\n",
    "N_views = 1\n",
    "for idx_x in range(N_views):\n",
    "    rad_x = idx_x*2*torch.pi/N_views\n",
    "    \n",
    "    rotation = torch.tensor([[rad_x, rot_y , rot_z]], device=device) \n",
    "\n",
    "    # ğŸ“¸ Also note that DiffDRR can take many representations of SO(3) ğŸ“¸\n",
    "    # For example, quaternions, rotation matrix, axis-angle, etc...\n",
    "    img = drr(rotation, translation, parameterization=\"euler_angles\", convention=\"ZYX\")\n",
    "    ### here use your favorite tool to save the created xray images.\n",
    "    ##visualisation\n",
    "    plt.figure()\n",
    "    plot_drr(img, ticks=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6139af0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann=scan.annotations ## annotation in scan\n",
    "### Loop over annotations for CT volumes\n",
    "for idx, a in enumerate(ann):\n",
    "    ## create volume for the annotation\n",
    "    vol_ann = np.zeros(vol.shape)\n",
    "    vol_ann[a.bbox()] = 1  ### a.bbox() contains the gold standard (3D bounding box) of the localisation of each annotation \n",
    "    ##  vol_ann is a binary volume the same size as the CT images, where all pixels are zero outside the box containing the indications and 1 inside the box\n",
    "    #\n",
    "    ## create drr object for the volume of the anotation\n",
    "    ## create drr object from CT\n",
    "    drr = DRR(\n",
    "        vol_ann,   # The binary volume with the annotations\n",
    "        spacing,   # Voxel dimensions of the volume\n",
    "        sdr=sdr,   # Source-to-detector radius (half of the source-to-detector distance)\n",
    "        height=height,  # Height of the DRR (if width is not seperately provided, the generated image is square)\n",
    "        delx=delx,    # Pixel spacing (in mm)\n",
    "    ).to(device)\n",
    "    #\n",
    "    ## generates the xray images \n",
    "    for idx_x in range(N_views):\n",
    "        rad_x = idx_x*2*torch.pi/N_views\n",
    "        print(rad_x)\n",
    "        rotation = torch.tensor([[rad_x, rot_y , rot_z]], device=device) \n",
    "\n",
    "        # ğŸ“¸ Also note that DiffDRR can take many representations of SO(3) ğŸ“¸\n",
    "        # For example, quaternions, rotation matrix, axis-angle, etc...\n",
    "        img = drr(rotation, translation, parameterization=\"euler_angles\", convention=\"ZYX\")\n",
    "        img[img>0] = 1\n",
    "        ### here use your favorite tool to save the created xray images s.\n",
    "        ##visualisation\n",
    "        plt.figure()\n",
    "        plot_drr(img, ticks=False)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7d9ef8",
   "metadata": {},
   "source": [
    "### Investigate the rotations and transformations of DRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6f650bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pyvista\n",
    "import torch\n",
    "\n",
    "from diffdrr.data import load_example_ct\n",
    "from diffdrr.drr import DRR\n",
    "from diffdrr.pose import convert\n",
    "from diffdrr.visualization import drr_to_mesh, img_to_mesh, labelmap_to_mesh, plot_drr\n",
    "\n",
    "pyvista.start_xvfb()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "     \n",
    "\n",
    "# Read in the CT volume\n",
    "subject = load_example_ct()\n",
    "\n",
    "# Make a mesh from the CT volume\n",
    "# ct = drr_to_mesh(subject, \"surface_nets\", threshold=225, verbose=True)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca70130e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read in the CT volume\n",
    "subject = load_example_ct()\n",
    "\n",
    "# # Make a mesh from the CT volume\n",
    "# ct = drr_to_mesh(subject, \"surface_nets\", verbose=True)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "919505f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DRR module for generating synthetic X-rays\n",
    "drr = DRR(subject, sdd=1020.0, height=200, delx=2.0).to(device)\n",
    "\n",
    "# Make a pose\n",
    "rot = torch.tensor([[45.0, 30.0, 0.0]], device=device) / 180 * torch.pi\n",
    "xyz = torch.tensor([[0.0, 800.0, 0.0]], device=device)\n",
    "pose = convert(rot, xyz, parameterization=\"euler_angles\", convention=\"ZXY\")\n",
    "# plot_drr(drr(pose))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962a13e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ct = drr_to_mesh(subject, \"surface_nets\", threshold=225, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7775add2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_590125/2088177000.py\", line 2, in <module>\n",
      "    camera, detector, texture, principal_ray = img_to_mesh(drr, pose)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/diffdrr/visualization.py\", line 296, in img_to_mesh\n",
      "    img = drr(pose, calibration)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/diffdrr/drr.py\", line 156, in forward\n",
      "    img = self.render(self.density, source, target, mask_to_channels, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/diffdrr/drr.py\", line 179, in render\n",
      "    img = self.renderer(\n",
      "          ^^^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/diffdrr/renderers.py\", line 44, in forward\n",
      "    alphas = _get_alphas(\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/diffdrr/renderers.py\", line 111, in _get_alphas\n",
      "    alphas = _filter_intersections_outside_volume(alphas, source, target, dims, eps)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/diffdrr/renderers.py\", line 119, in _filter_intersections_outside_volume\n",
      "    alphas = alphas[..., good_idxs.any(dim=[0, 1])]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: any() received an invalid combination of arguments - got (dim=list, ), but expected one of:\n",
      " * ()\n",
      "      didn't match because some of the keywords were incorrect: dim\n",
      " * (int dim, bool keepdim)\n",
      " * (name dim, bool keepdim)\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/stack_data/core.py\", line 677, in included_pieces\n",
      "    scope_pieces = self.scope_pieces\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/stack_data/core.py\", line 614, in scope_pieces\n",
      "    scope_start, scope_end = self.source.line_range(self.scope)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ensai/anaconda3/envs/rxtool/lib/python3.11/site-packages/stack_data/core.py\", line 178, in line_range\n",
      "    return line_range(self.asttext(), node)\n",
      "                      ^^^^^^^^^^^^\n",
      "AttributeError: 'Source' object has no attribute 'asttext'\n"
     ]
    }
   ],
   "source": [
    "# Make a mesh from the camera and detector plane\n",
    "camera, detector, texture, principal_ray = img_to_mesh(drr, pose)\n",
    "\n",
    "# Make the plot\n",
    "plotter = pyvista.Plotter()\n",
    "# plotter.add_mesh(ct)\n",
    "plotter.add_mesh(camera, show_edges=True, line_width=1.5)\n",
    "plotter.add_mesh(principal_ray, color=\"lime\", line_width=3)\n",
    "plotter.add_mesh(detector, texture=texture)\n",
    "\n",
    "# Render the plot\n",
    "plotter.add_axes()\n",
    "plotter.add_bounding_box()\n",
    "plotter.show_bounds(grid=\"front\", location=\"outer\", all_edges=True)\n",
    "\n",
    "# plotter.show()  # If running Jupyter locally\n",
    "# plotter.show(jupyter_backend=\"server\")  # If running Jupyter remotely\n",
    "plotter.export_html(\"render.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rxtool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
